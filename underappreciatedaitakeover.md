---
layout: page
title: Some underappreciated reasons why single AI might take over the world
permalink: /underappreciatedaitakeover
---

The following is a sketch of some ideas stemming from a [Twitter discussion](https://twitter.com/elliot_olds/status/1079176432327188480).

I currently think a "singleton AI" scenario is about 60% likely vs. a multi-polar scenario where several AIs gain significant power and none has the ability to dominate the world.

An important background assumption is that gaining power is an instrumental value for nearly all goals an AI could have.

### Context

A common objection to a single AI taking over is: the first group of humans to invent farming didn't take over the world. The first group of humans to invent industry didn't take over the world. In both of these cases their innovations spread throughout the world and were used by other groups to "catch up" to the original group far before the original group could amass enough power to control the world. 

Therefore, we should expect the first group to invent general AI to not be able to take over the world for similar reasons.

There are a few reasons why the agricultural and industrial revolutions were much harder for single groups to harness for world domination than an AI revolution would be. Below I compare the two previous revolutions to an AI revolution that begins when an AI starts to improve itself with such skill that the humans at the firm that created the AI are no longer doing useful engineering and all meaningful improvements to the AI are done by the AI itself.

### An AI is far less likely to leak information that weakens its relative power than systems made up of humans would be

**Humans have incentives that are misaligned with their firm's incentives, whereas sub-components of an AI would not be similarly misaligned**

Doing industrial stuff at scale requires lots of people, and these people have their own goals which make it in their interest to start their own firms or join other firms who are trying to hire them away. Individual workers at firms also have incentives to share information outside the firm to gain personal status. Some humans may be motivated to spread useful knowledge for the benefit of humanity as a whole, rather than acting for the benefit of their firm.

Because the AI is a unified entity (or lots of copies of entities with identical goals), it doesn't have sub-components whose goals involve compromising the AI's power. The AI will do what is best for the AI, and its sub-components will align with that goal. The AI will only sell knowledge to outsiders or brag about its insights if it helps the AI achieve its overall goal (it's hard to imagine it doing those things at all). The principle-agent problems that firms deal with don't exist for the AI.

**A self-improved AI would likely be illegible to humans**

Just like humans can't look at the connections between the neurons in a human brain and learn how to make something as intelligent, looking directly at an AI's deep neural network is similarly unlikely to give them insights into how to build something similarly intelligent but with different goals. (Humans could copy it, but then they'd just have another AI with the same goals and capabilities as the original).

If the AI was smarter than humans (picture it as John von Neumann operating at 100x speed) then it could alter its architecture without leaving copies of human readable source code or instructions laying around for humans to read. If it were intentionally trying to remain opaque, it likely wouldn't even store its neural network in a form that humans could recognize.

**Full goal alignment between sub-components of the AI plus a desire for power would result in opaque/deceptive behavior**

If something is rapidly gaining in power and trying to take over the world, it's likely strategically wise for it to hide its power and intentions from rivals while it grows in power.

An AI who wanted to gain power as an instrumental goal would be very interested in extracting valuable information from humans (who would be much less strategic than the AI, likely unaware that they were communicating with an AI or that it was attempting to take over the world), but it would not want to reveal anything that would reduce its relative power. It may mislead humans or point them toward technical local optima that look promising but are dead ends. 

The AI would essentially have access to "human culture" via the Internet and interacting with humans, but could be extremely cagey about preventing humans from even being aware of the much more powerful "AI culture" (described below). The humans that the AI gained knowledge from would probably not even know they were sharing info with the AI, as it's easy to pose as anyone online.

**Keeping important secrets for centuries is much harder than keeping them for months**

For humans to learn leaked insights from an AI, these insights would have to become legible to humans and to flow through human networks (since by definition we're considering the first smarter than human AI). There is an upper bound on how fast humans can usefully piece together and spread insights from leaked clues. 

In the "industrial era", the economy doubles roughly every 15 years. If the first group to invent an industrial firm wanted to keep their industrial insights secret so they could take over the world, they'd have to keep this secret for ~10 doublings (or ~150 years) to be as powerful as the rest of the world (if they started at 1/1000th as powerful as the rest of the world). This is a very long time for humans to keep such secrets. In contrast, if doubling times in the 'AI era' were months or weeks an AI would need to keep secrets for a much shorter period of time.

### Human culture is no match for AI culture

A skeptic might say: "for humans to innovate it's really important for them to talk to each other. For this new AI to innovate it will need to talk to humans so that it can get these knowledge-sharing gains. During the course of this talking, the AI will end up sharing information which humans will use to create more AIs that keep the original AI from dominating".

**Principle agent problems, again**

One part of why this is unlikely is the results from the previous section. When humans in different firms talk to each other there are all sorts of principal agent problems. The humans are trying to benefit themselves by impressing others, getting other firms to give them more money, starting their own firms, etc. Their ultimate loyalty is not to the firm that currently employs them. The situation is different for the AI for reasons discussed above. 

**Human culture is really bad at the problem it tries to solve**

Compared to what seems theoretically possible, human culture is awful. Culture is how humanity transmits and preserves knowledge. Yet our system of getting things from one human brain to another is extremely slow, inefficient, and lossy. Producing another human is costly compared to making a copy of software. Educating humans takes a long time (again compared to making copies of the best version of an AI). The mechanism of getting concepts from one human head into another one through language is imprecise. Humans are also tribal and believe things for status / signalling reasons, so they are prone to resisting being educated.

**AI culture would be far more powerful**

Analogous to human culture, "AI culture" refers to how sub-components of the AI or copies of the AI would share and preserve knowledge.

Imagine you've made an AI as smart as John von Neumann operating at 100x speed. Then it makes 10,000 copies of itself to go off and learn things and collaborate with each other. (For now we'll assume there isn't a better achievable architecture than 10,001 copies of the same AI. If there is, we should worry even more). 

The culture between these copies seems likely to be far better than human culture. The AI copies would all have the same goal so their communication with each other would look like what Aumann's Agreement Theorem implies. 

Because the AI exists entirely in the digital realm and can self-modify, the AI instances would likely share knowledge in a way more like installing a software patch vs. human methods of knowledge transmission. 

To get an intuitive sense of how powerful this type of fast and aligned digital information sharing could be: imagine if everyone in the world immediately knew anything that any other human learned, with the same clarity of the person who understood it best. 

Even if an AI still had significant communication overhead between its components, the amount of improvement possible above the current human situation would likely be enormous.

### How 'lumpy' would AI progress be?

A skeptic may still object that I'm assuming it's plausible that some AI with the intelligence of John von Neumann might be created and be the only smarter-than-human AI in existence for some time, without lots of other almost-as-smart AIs existing.

**'Dumb human' --> 'Very smart human' isn't that big of a jump**

A popular argument is that a jump from AIs at around the 90 IQ level to those at around the 160 IQ level isn't really that big of a jump. It seems big to us because we're used to zooming in on differences between humans. But if you look at the intelligence level of all things with brains, this isn't that huge of a leap. However is a plausible counter-argument to this claim [here](https://aiimpacts.org/making-or-breaking-a-thinking-machine/).

**The step can still be smooth / non-lumpy within a team but appear lumpy outside of that team**

My claim is not that a team of AI researchers will one day only be able to create AIs with 90 IQ and then with one simple step be able to create them with 160 IQ. The process within a human team could over the course of months. This would still result in a singleton AI scenario if the firm didn't immediately share its technique for creating this big improvement outside of the firm. It could also result in a singleton AI scenario even if the firm immediately shared its techniques but it took other teams months to sort out the implicit knowledge / infrastructure to make the techniques work.

To put it another way: imagine that DeepMind eventually produces the first smarter than human AI. How likely is it that their closest competitor will create smarter than human AI within a couple months if DeepMind doesn't try to share their discovery? How likely is it that DeepMind willingly shares the insights that allowed them to do this with the outside world (or even with a handful of competing AI organizations like OpenAI) within the first few months of their innovation? They may fail to share this info because they think they take AI safety more seriously than other organizations, and are better at controlling AIs. 

As described in earlier sections, humans groups aren't that good at keeping secrets, but the AI may only need a few weeks or months of a head start given how much more powerful AI culture would be than human culture.