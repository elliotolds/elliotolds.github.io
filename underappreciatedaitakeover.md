---
layout: page
title: Underappreciated reasons why a single AI might take over the world
permalink: /underappreciatedaitakeover
---

_The following is a sketch of some ideas stemming from a [Twitter discussion](https://twitter.com/elliot_olds/status/1079176432327188480). It's unfinished and thus less polished than the other essays on this site._

I currently think a "singleton AI" scenario where one supperintelligent AI becomes more powerful than the rest of the world combined is about 60% likely.

An important background assumption which I don't go into here is that gaining power is an instrumental value for nearly all goals an AI could have. 

### Context

A common objection to a single AI taking over is: the first group of humans to invent farming didn't take over the world. The first group of humans to invent industry didn't take over the world. In both of these cases their innovations spread throughout the world and were used by other groups to "catch up" to the original group far before the original group could amass enough power to control the world. Therefore, we should expect the first group to invent general AI to not be able to take over the world for similar reasons.

There are a few reasons why the agricultural and industrial revolutions were much harder for single groups to harness for world domination than an AI revolution would be. Below I compare the two previous revolutions to an AI revolution that begins when an AI starts to improve itself with such skill that the humans at the firm that created the AI are no longer doing useful engineering and all meaningful improvements to the AI are done by the AI itself.

### An AI is far less likely to leak information that weakens its relative power than systems made up of humans would be

**Humans have incentives that are misaligned with their firm's incentives, whereas sub-components of an AI would not be similarly misaligned**

Doing industrial stuff at scale requires lots of people, and these people have their own goals which make it in their interest to start their own firms or join other firms who are trying to hire them away. Individual workers at firms also have incentives to share information outside the firm to gain status. Some humans may be motivated to spread useful knowledge for the benefit of humanity as a whole, rather than acting for the benefit of their firm.

Because the AI is a unified entity (or lots of copies of entities with identical goals), it doesn't have sub-components whose goals involve compromising the AI's power. The AI will do what's best for the AI, and its sub-components will align with that goal. The AI will only sell important knowledge to outsiders or brag about its insights if it helps the AI achieve its overall goal (and it's hard to imagine it doing those things at all). The principle-agent problems that firms deal with don't exist for the AI.

**A self-improved AI would likely be illegible to humans**

Just like humans can't look at the connections between the neurons in a human brain and learn how to make something as intelligent, looking directly at an AI's deep neural network is similarly unlikely to give them insights into how to build something similarly intelligent but with different goals. (Humans could copy it, but then they'd just have another AI with the same goals and capabilities as the original).

If the AI was smarter than humans (picture it as John von Neumann operating at 100x speed) then it could alter its architecture without leaving copies of human readable source code or instructions laying around for humans to read. If it were intentionally trying to remain opaque, it likely wouldn't even store its neural network in a form that humans could recognize.

**Full goal alignment between sub-components of the AI plus a desire for power would result in opaque/deceptive behavior**

If something is rapidly gaining in power and trying to take over the world, it's likely strategically wise for it to hide its power and intentions from rivals while it grows in power.

An AI who wanted to gain power as an instrumental goal would be very interested in extracting valuable information from humans (who would be much less strategic than the AI, likely unaware that they were communicating with an AI or that it was attempting to take over the world). Yet the AI would be very cagey about revealing anything that reducd its relative power. 

The AI would essentially have access to "human culture" via the Internet and interacting with humans, but humans would not even be awayre of the new and much more powerful "AI culture" (described below). 

**Keeping important secrets for centuries is much harder than keeping them for months, years, or decades**

We saw above than a superintelligent AI would be much less likely to leak information than a modern firm. However even if an AI did leak useful information there's an upper bound on how fast humans can piece together insights from leaked clues, spread these insights, and react effectively.

Since the industrial revolution the economy has doubled roughly every 15 years. If the first group to invent an industrial firm wanted to keep their industrial insights secret so they could take over the world, they'd have to keep this secret for ~10 doublings (or ~150 years) to be as powerful as the rest of the world (assuming they started at 1/1000th as powerful as the rest of the world). In contrast, doubling times of a superintelligence will likely be much shorter.

How fast a superintelligence could increase its power is a [hotly debated topic](https://aiimpacts.org/will-ai-see-sudden-progress/) and is out of scope for this essay. Regardless of your views on the topic, the arguments in this section aim to show that the minimum required speed of self-improvement for an AI to take over the world is slower than you might otherwise think.

### Human culture is no match for AI culture

A skeptic might say: "for humans to innovate it's really important for them to talk to each other. For this new AI to innovate it will need to talk to humans so that it can get these knowledge-sharing gains. During the course of this talking, the AI will end up sharing information which humans will use to create more AIs that keep the original AI from dominating".

**Principle agent problems, again**

One part of why this is unlikely are the results from the previous section. When humans in different firms talk to each other there are all sorts of principal agent problems. The humans are trying to benefit themselves by impressing others, getting other firms to give them more money, starting their own firms, etc. Their ultimate loyalty is not to the firm that currently employs them. The situation is different for an AI for reasons discussed above. 

**Human culture is really bad at the problem it tries to solve**

Compared to what seems theoretically possible, human culture is awful. Culture is how humans transmit and preserve knowledge. Yet our system of getting things from one human brain to another is extremely slow, inefficient, and lossy. Producing another human is costly compared to making a copy of software. Educating humans takes a long time (again compared to making copies of the best version of an AI). The mechanism of getting concepts from one human head into another one through language is imprecise. Humans are also tribal and believe things for status / signalling reasons, so they tend to resist updating their beliefs when presented with new evidence.

**AI culture would be far more powerful**

Analogous to human culture, "AI culture" refers to how sub-components of the AI or copies of the AI would share and preserve knowledge.

Imagine you've made an AI as smart as John von Neumann operating at 100x speed. Then it makes 10,000 copies of itself to go off and learn things and collaborate with each other. (For now we'll assume there isn't a better achievable architecture than 10,001 copies of the same AI. If there is, we should worry even more). 

The culture between these copies seems likely to be far better than human culture. The AI copies would all have the same goal so their communication with each other would look like what [Aumann's Agreement Theorem](https://www.scottaaronson.com/blog/?p=2410) implies. 

Because the AI exists entirely in the digital realm and can self-modify, the AI instances would likely share knowledge in a way more like installing a software patch vs. human methods of knowledge transmission. 

To get an intuitive sense of how powerful this type of fast and aligned digital information sharing could be: imagine if everyone in the world immediately knew anything that any other human learned, with the same clarity of the person who understood it best. 

Even if an AI still had significant communication overhead between its components, the amount of improvement possible above the current human situation would likely be enormous.

### How 'lumpy' would AI progress be?

A skeptic may still object that I'm assuming it's plausible that some AI with the intelligence of John von Neumann might be created and be the only smarter-than-human AI in existence for some time, without lots of other almost-as-smart AIs existing.

<!---
**'Dumb human' to 'Very smart human' isn't that big of a jump**

A popular argument is that a jump from AIs at around the 90 IQ level to those at around the 160 IQ level isn't really that big of a jump. It seems big to us because we're used to zooming in on differences between humans. But if you look at the intelligence level of all things with brains, this isn't that huge of a leap. However is a plausible counter-argument to this claim [here](https://aiimpacts.org/making-or-breaking-a-thinking-machine/).
-->

**The step can still be smooth / non-lumpy within a team but appear lumpy outside of that team**

Lumpy progress within a team might look like this: a team of AI researchers is able to create an AI with an IQ of 90 on Monday. After one modification they're able to create an AI with an IQ of 160 on Tuesday. 

Even if it took a team of researchers a year to go from a 90 IQ AI to a 160 IQ AI, what matters is the delay between when the first team deploys their AI and when the next fastest team deploys something similar. If this delay between teams is close to the time required for a superintelligence to gain a decisive strategic advantage, this could result in a single AI taking over the world. 

Even if the team who deployed the first superintelligent AI immediately revealed what they had done and shared their techniques with other teams, it may take these other teams months to sort out the implicit knowledge / infrastructure to make the techniques work.

However the first team to deploy a superintelligent AI may not reveal that they've done so. It may be considered dangerous and they may fear the reaction from governments and the rest of the AI community. They may privately believe in the goodness of their AI and want it to take over the world. 

The first team to deploy a superintelligent AI may not even realize what they've done. The AI may have good reason not to call attention to itself if it knew that it was not supposed to have been released.

Other teams might have the capability of deploying similarly intelligent AIs but choose not do so because they're more responsible than the team that released the AI. This situation may persist for years while the initial AI builds up its capabilities in secret. 

<!---
To put it another way: imagine that DeepMind eventually produces the first smarter than human AI. How likely is it that their closest competitor will create smarter than human AI within a couple months if DeepMind doesn't try to share their discovery? How likely is it that DeepMind willingly shares the insights that allowed them to do this with the outside world (or even with a handful of competing AI organizations like OpenAI) within the first few months of their innovation? They may fail to share this info because they think they take AI safety more seriously than other organizations, and are better at controlling AIs. 
-->

As described in earlier sections, humans groups aren't that good at keeping secrets, but the AI may not need that long of a head start given how much more powerful AI culture would be than human culture.